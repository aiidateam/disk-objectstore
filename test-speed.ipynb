{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24e4fa0-45ab-4511-bd74-8b4f015907ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import disk_objectstore as dos\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c7aa9f-691c-41ca-bc58-c5c7b1184a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bytes(size):\n",
    "    return b''.join(numpy.random.choice([str(_).encode() for _ in range(10)], size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d34242-eb6c-465d-b723-8e6ec0ba929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = get_bytes(100_000_000)\n",
    "\n",
    "with open('test.binary.gz', 'wb') as fh:\n",
    "    fh.write(zlib.compress(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2da7d545-9775-4c6e-8b39-1e3b8dfc185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readline_SLOW(self, size=-1):\n",
    "    res = bytearray()\n",
    "    while size < 0 or len(res) < size:\n",
    "        b = self.read(1)\n",
    "        if not b:\n",
    "            break\n",
    "        res += b\n",
    "        if res.endswith(b\"\\n\"):\n",
    "            break\n",
    "    return bytes(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14661f6d-bdba-4230-955b-f839463b4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readline_WITH_PEEK(self, size=-1):\n",
    "    r\"\"\"Read and return a line of bytes from the stream.\n",
    "\n",
    "    If size is specified, at most size bytes will be read.\n",
    "    Size should be an int.\n",
    "\n",
    "    The line terminator is always b'\\n' for binary files; for text\n",
    "    files, the newlines argument to open can be used to select the line\n",
    "    terminator(s) recognized.\n",
    "    \"\"\"\n",
    "    # For backwards compatibility, a (slowish) readline().\n",
    "    if hasattr(self, \"peek\"):\n",
    "        def nreadahead():\n",
    "            readahead = self.peek(1)\n",
    "            if not readahead:\n",
    "                #print('peek 1')\n",
    "                return 1\n",
    "            n = (readahead.find(b\"\\n\") + 1) or len(readahead)\n",
    "            if size >= 0:\n",
    "                n = min(n, size)\n",
    "            #print(f'peek {n}')\n",
    "            return n\n",
    "    else:\n",
    "        def nreadahead():\n",
    "            #print(f'nopeek 1')\n",
    "            return 1\n",
    "    \n",
    "    res = bytearray()\n",
    "    while size < 0 or len(res) < size:\n",
    "        b = self.read(nreadahead())\n",
    "        if not b:\n",
    "            break\n",
    "        res += b\n",
    "        if res.endswith(b\"\\n\"):\n",
    "            break\n",
    "    return bytes(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "079e3311-850c-4b91-8dd6-eeeb89d91669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readline(self, size=-1):\n",
    "    r\"\"\"Read and return a line of bytes from the stream.\n",
    "\n",
    "    If size is specified, at most size bytes will be read.\n",
    "    Size should be an int.\n",
    "\n",
    "    The line terminator is always b'\\n' for binary files; for text\n",
    "    files, the newlines argument to open can be used to select the line\n",
    "    terminator(s) recognized.\n",
    "    \"\"\"\n",
    "    \n",
    "    res = bytearray()\n",
    "    while size < 0 or len(res) < size:\n",
    "        # Read for a number of bytes equal to the size of the internal buffer\n",
    "        # (so no read on disk are needed). However, if it the buffer is empty\n",
    "        # read at least 1 byte. This will in pracice read another chunk,\n",
    "        # so at the next iteration, we have a much bigger read ahead)\n",
    "        bytes_to_read = max(1, len(self._internal_buffer))\n",
    "        b = self.read(bytes_to_read)\n",
    "        if not b:\n",
    "            break\n",
    "        res += b\n",
    "        if res.endswith(b\"\\n\"):\n",
    "            break\n",
    "    return bytes(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90cf63a6-ff9d-4c9c-abd7-9621f30a950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17302512493915856\n"
     ]
    }
   ],
   "source": [
    "with open('test.binary.gz', 'rb') as fh:\n",
    "    t = time.monotonic()\n",
    "    #data = zlib.decompress(fh.read())\n",
    "    stream = dos.utils.ZlibStreamDecompresser(fh)\n",
    "    data = stream.read()\n",
    "    print(time.monotonic() - t)\n",
    "\n",
    "assert data == content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a557c9e-623c-42d1-b3e5-7cdacfe813e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Peekable(dos.utils.ZlibStreamDecompresser):\n",
    "    #def peek(self, size):\n",
    "    #    want = min(size, len(self._internal_buffer))\n",
    "    #    #return self._internal_buffer[:want]\n",
    "    #    return self._internal_buffer\n",
    "\n",
    "    def _read_compressed(self, size: int = -1) -> bytes:\n",
    "        \"\"\"\n",
    "        Read and return up to n bytes.\n",
    "\n",
    "        If the argument is omitted, None, or negative, reads and\n",
    "        returns all data until EOF (that corresponds to the length specified\n",
    "        in the __init__ method).\n",
    "\n",
    "        Returns an empty bytes object on EOF.\n",
    "\n",
    "        Note that this should be used only internally, as this function\n",
    "        always reads from the compressed stream, but the position\n",
    "        (seek) in the compressed stream will be wrong/outdated once\n",
    "        an uncompressed stream is set!\n",
    "\n",
    "        TODO: add method to reset the uncompressed stream (close it if not\n",
    "        closed, set internally variable to False, seek back to zero)\n",
    "        \"\"\"\n",
    "        if size is None or size < 0:\n",
    "            # Read all the rest: we call ourselves but with a length,\n",
    "            # and return the joined result\n",
    "            data = []\n",
    "            while True:\n",
    "                next_chunk = self.read(self._CHUNKSIZE)\n",
    "                if not next_chunk:\n",
    "                    # Empty returned value: EOF\n",
    "                    break\n",
    "                data.append(next_chunk)\n",
    "            # Making a list and joining does many less mallocs, so should be faster\n",
    "            return b\"\".join(data)\n",
    "\n",
    "        if size == 0:\n",
    "            return b\"\"\n",
    "\n",
    "        while len(self._internal_buffer) < size:\n",
    "            old_unconsumed = self._decompressor.unconsumed_tail\n",
    "            next_chunk = self._compressed_stream.read(\n",
    "                max(0, self._CHUNKSIZE - len(old_unconsumed))\n",
    "            )\n",
    "\n",
    "            # In the previous step, I might have some leftover data\n",
    "            # since I am using the max_size parameter of .decompress()\n",
    "            compressed_chunk = old_unconsumed + next_chunk\n",
    "            # The second parameter is max_size. We know that in any case we do\n",
    "            # not need more than `size` bytes. Leftovers will be left in\n",
    "            # .unconsumed_tail and reused a the next loop\n",
    "            while len(self._internal_buffer) < size:\n",
    "                # I do another while loop, as I want also to decompress in chunks\n",
    "                # (this time, of uncompressed data).\n",
    "                # I continue either until I get to the required size,\n",
    "                # or if there is no more data to decompress: then I break from\n",
    "                # this internal loop, so I can read another chunk of *compressed*\n",
    "                # data in the outer loop.\n",
    "                try:\n",
    "                    decompressed_chunk = self._decompressor.decompress(\n",
    "                        # Here I still limit what I decompress.\n",
    "                        # I want to possibly still decompress a bit more than I need, because\n",
    "                        # e.g. this will allow the `peek()` method to return more bytes, making\n",
    "                        # the implementation of `readline()` efficient (otherwise, it would\n",
    "                        # iterate one byte at a time, and would be impossibly slow for large\n",
    "                        # objects).\n",
    "                        # However, I still put a limit to self._CHUNKSIZE to still have a hard\n",
    "                        # limit on the amount of bytes I put in memory.\n",
    "                        # This is e.g. to protect from a very huge (say 1TB) compressed file \n",
    "                        # of the same byte character: this would compress down to a very small\n",
    "                        # compressed size of a few bytes, but when decompressing, would\n",
    "                        # fill up the memory. In this way, instead, I can only decompress\n",
    "                        # up to self._CHUNKSIZE *decompressed* bytes: I avoid memory issues,\n",
    "                        # and the implementation of `readline` is still quite efficient.\n",
    "                        compressed_chunk, self._CHUNKSIZE\n",
    "                    )\n",
    "                except self.decompress_error as exc:\n",
    "                    raise ValueError(\"Error while uncompressing data\") from exc\n",
    "                if not decompressed_chunk:\n",
    "                    break\n",
    "                self._internal_buffer += decompressed_chunk\n",
    "\n",
    "            if not next_chunk and not self._decompressor.unconsumed_tail:\n",
    "                # Nothing to do: no data read, and the unconsumed tail is over.\n",
    "                if self._decompressor.eof:\n",
    "                    # Compressed file is over. We break\n",
    "                    break\n",
    "                raise ValueError(\n",
    "                    \"There is no data in the reading buffer, but we didn't reach the end of \"\n",
    "                    \"the compressed stream: there must be a problem in the incoming buffer\"\n",
    "                )\n",
    "\n",
    "        # Note that we could be here also with len(self._internal_buffer) < size,\n",
    "        # if we used 'break' because the internal buffer reached EOF.\n",
    "        to_return, self._internal_buffer = (\n",
    "            self._internal_buffer[:size],\n",
    "            self._internal_buffer[size:],\n",
    "        )\n",
    "        self._pos += len(to_return)\n",
    "\n",
    "        return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc23474d-68ac-4673-90e7-7596a6807fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1724113749805838\n"
     ]
    }
   ],
   "source": [
    "with open('test.binary.gz', 'rb') as fh:\n",
    "    t = time.monotonic()\n",
    "    #data = zlib.decompress(fh.read())\n",
    "    stream = Peekable(fh)\n",
    "    data = stream.read()\n",
    "    print(time.monotonic() - t)\n",
    "\n",
    "assert data == content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb36e541-9710-4467-99e3-9fe4e78f31a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19443516700994223\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('test.binary.gz', 'rb') as fh:\n",
    "    t = time.monotonic()\n",
    "    stream = Peekable(fh)\n",
    "    data = readline(stream)\n",
    "    print(time.monotonic() - t)\n",
    "\n",
    "print(len(stream._internal_buffer))\n",
    "print(len(stream._decompressor.unconsumed_tail))\n",
    "\n",
    "assert data == content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc0bfcc7-8b8b-4909-b56b-769e6af03754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.peek(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da9ce4-06d3-47b0-ae9e-4a249914855f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
